{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae-su-kim/vllm-101/blob/main/samples/colab_vllm_integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1GnTb5A5_OL"
      },
      "source": [
        "# vLLM 실습 101\n",
        "\n",
        "이 노트북에서는 vLLM library의 셋업 및 사용 방법을 간단히 알아봅니다.\n",
        "\n",
        "## COLAB runtime 설정 (user action required)\n",
        "\n",
        "이 노트북은 LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct 모델을 vllm library를 이용해서 실행해보도록 작성되었습니다. 이 노트북은 Google Colab에서 제공하는 GPU 환경을 사용하는 것을 전제로 합니다.\n",
        "\n",
        "먼저 다음과 같이 Colab Runtime을 설정합니다.\n",
        "\n",
        "메뉴 -> 런타임 -> 런타임 유형 변경 -> T4 GPU (무료 환경 사용 가능)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp2ZVNJx9NqY"
      },
      "source": [
        "# 1. vLLM 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZQvt8SN5_ON"
      },
      "outputs": [],
      "source": [
        "!pip install vllm==0.7.3 requests datasets\n",
        "\n",
        "# When running vllm directly from source, use this instead\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# import sys\n",
        "# import os\n",
        "# sys.path.append(os.path.abspath('..'))\n",
        "## os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E68l918G5_ON"
      },
      "outputs": [],
      "source": [
        "## Login to huggingface_hub if we need gated model. Not applicable now.\n",
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMAaj63P5_OO"
      },
      "source": [
        "# 2. LLM 모델 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zun_Ohpj5_OO"
      },
      "outputs": [],
      "source": [
        "from vllm import LLM\n",
        "\n",
        "llm = LLM(model='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUxS0wvV5_OO"
      },
      "source": [
        "이 셀이 성공적으로 실행되었다면, vLLM 설정과 huggingface 접근이 잘 수행된 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fGxqHMb-TaF"
      },
      "source": [
        "추가로 현재 노트북에서 display를 개선하기 위한 코드를 실행합니다 (vLLM과 관련 없음)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFWpmVa75_OO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def display_header(text):\n",
        "    display(Markdown(f'**{text}**'))\n",
        "\n",
        "def display_content(text):\n",
        "    display(Markdown(f'```\\n{text}\\n```'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfL4Qcjw5_OO"
      },
      "source": [
        "## 해당 언어 모델을 위한 시스템 프롬프트 준비\n",
        "이 시스템 프롬프트는 [LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct) 모델의 repository에서 예시로 사용하고 있는 프롬프트입니다. 프롬프트는 활용에 따라서 자유롭게 수정할 수 있고, 많은 경우에 실제 유저 경험에도 큰 영향을 미치게 됩니다. 자유롭게 변경해보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McNRXsOm5_OP"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\\\n",
        "You are EXAONE model from LG AI Research, a helpful assistant.\\\n",
        "\"\"\"\n",
        "\n",
        "def get_prompt(message: str, system_prompt: str = SYSTEM_PROMPT) -> str:\n",
        "    return f'<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{message} [/INST]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvgPz94F5_OP"
      },
      "source": [
        "## Offline inference 테스트\n",
        "\n",
        "위에서 생성한 vllm.LLM 인스턴스를 통해 LLM 모델의 offline inference를 실험해볼 수 있습니다. Offline inference는 API 서빙을 위한 batching 등 front-end 기능을 제외한 것으로, LLM inference 자체를 실험 해 보고 싶을때, 혹은 로컬 스크립트에서 LLM 기능이 필요할때 사용할 수 있습니다.\n",
        "\n",
        "[SamplingParams](https://docs.vllm.ai/en/v0.6.0/dev/sampling_params.html)는 llm 모델의 출력 토큰 생성시에 sampling 과정을 정의할 수 있는 변수로, top_p, top_k, temperature 등 여러 파라미터를 수정하여 output을 바꿀 수 있습니다. 해당 옵션들은 서빙 성능에도 영향이 있고, 모델 성능에도 영향이 있으므로 활용처에 따라 다르게 세팅할 필요가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neXEbPN55_OP"
      },
      "outputs": [],
      "source": [
        "from vllm import SamplingParams\n",
        "\n",
        "sampling_params = SamplingParams()\n",
        "sampling_params.max_tokens = 100\n",
        "sampling_params.top_p = 0.5\n",
        "sampling_params.top_k = 10\n",
        "sampling_params.temperature = 0.7\n",
        "\n",
        "question = \"판교역에 대해 알려줘.\"\n",
        "prompt = get_prompt(question)\n",
        "\n",
        "display_header(\"Prompt:\")\n",
        "display_content(prompt)\n",
        "\n",
        "display_header(\"Answer:\")\n",
        "result = llm.generate(prompt, sampling_params=sampling_params)\n",
        "display_content(result[0].outputs[0].text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpzAPjLa5_OP"
      },
      "source": [
        "결과가 성공적으로 출력되었다면, vLLM의 기본적인 사용에 성공하셨습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaU58LRc5_OP"
      },
      "source": [
        "## Online Serving 테스트\n",
        "\n",
        "Offline Serving에 성공하였으니, 이제 Online Serving API를 띄워봅니다. LLM의 Online Serving은 보통 OpenAI API-compatible한 포맷으로 지원하는 경우가 많으며, vllm에서도 이를 지원하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "vJHiI4OR5_OP"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# del LLM\n",
        "\n",
        "# Start vllm server in the background\n",
        "vllm_process = subprocess.Popen([\n",
        "    'vllm',\n",
        "    'serve',  # Subcommand must follow vllm\n",
        "    'LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct',\n",
        "    '--trust-remote-code',\n",
        "    '--dtype', 'half',\n",
        "    '--max-model-len', '2048',\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewGh5tT75_OP"
      },
      "source": [
        "## vLLM API 상태 확인하기\n",
        "\n",
        "서빙 API를 subprocess로 실행하였으므로 해당 프로세스에 직접 HTTP 요청을 통해 상태를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q78rcsAl5_OP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def check_vllm_status():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"vllm server is running\")\n",
        "            return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"vllm server is not running\")\n",
        "        return False\n",
        "\n",
        "try:\n",
        "    # Monitor the process\n",
        "    while True:\n",
        "        if check_vllm_status() == True:\n",
        "            print(\"The vllm server is ready to serve.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"The vllm server is not responding.\")\n",
        "        time.sleep(5)  # Check every second\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping the check of vllm...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZepO0CNrMSeG"
      },
      "source": [
        "## 서빙 성능 벤치마크 해보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnfEAS6aQV__"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpM9ZgdBMZYC"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vllm-project/vllm\n",
        "!mkdir /content/data && cd /content/data && wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SyCJY2pNpDP",
        "outputId": "9bafbdcf-34ef-48db-ccd7-070606b44ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-25 03:28:42.552383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742873322.809273   12677 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742873322.878753   12677 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-25 03:28:43.416775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 03-25 03:29:03 __init__.py:207] Automatically detected platform cuda.\n",
            "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=256, random_output_len=256, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\n",
            "Starting initial single prompt test run...\n",
            "Initial test run completed. Starting main benchmark run...\n",
            "Traffic request rate: inf\n",
            "Burstiness factor: 1.0 (Poisson process)\n",
            "Maximum request concurrency: None\n",
            "  0% 0/100 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "!cd /content/vllm && python benchmarks/benchmark_serving.py \\\n",
        "                        --backend vllm \\\n",
        "                        --model LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct \\\n",
        "                        --dataset-name random \\\n",
        "                        --random-input-len 256 \\\n",
        "                        --random-output-len 256 \\\n",
        "                        --num-prompts 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 프로파일링 해보기\n",
        "\n",
        "torch profiler를 활용하여 vLLM을 프로파일링 해볼 수 있습니다. 아래의 코드를 실행하여 profiler를 킨 채로 offline inference를 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# delete previous LLM instance\n",
        "try:\n",
        "    del llm\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "os.environ[\"VLLM_TORCH_PROFILER_DIR\"] = \"./vllm_profile\"\n",
        "llm = LLM(model='LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct')\n",
        "\n",
        "sampling_params = SamplingParams()\n",
        "sampling_params.max_tokens = 10\n",
        "sampling_params.top_p = 0.5\n",
        "sampling_params.top_k = 10\n",
        "sampling_params.temperature = 0.7\n",
        "\n",
        "llm.start_profile()\n",
        "_ = llm.generate(\"판교역에 대해 알려줘.\", sampling_params=sampling_params)\n",
        "llm.stop_profile()\n",
        "time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "위의 코드 실행 이후, 파일 탭에서 vllm_profile 디렉토리 내에 프로파일링 결과 파일이 생성된 것을 확인할 수 있습니다.\n",
        "파일 탭에서 우클릭하여 해당 파일을 다운로드 할 수 있으며, 다운로드 이후 [perfetto](https://ui.perfetto.dev/)를 이용해 열어볼 수 있습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
